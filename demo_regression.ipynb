{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Demo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up global variables and random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from dataoob.util import set_random_state\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "random_state = set_random_state(10)\n",
    "date = datetime.now().strftime(\"%m-%d_%H:%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick Noise rate and data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_rate = .1\n",
    "regression_datasets = [\"diabetes\", \"linnerud\"]\n",
    "dataset_name = regression_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataoob.dataloader import DataFetcher, add_gauss_noise\n",
    "\n",
    "# Equivalent arguments\n",
    "fetcher = (\n",
    "    DataFetcher(dataset_name, False, random_state)\n",
    "    .split_dataset(100, 50, 50)\n",
    "    .noisify(add_gauss_noise, noise_rate=noise_rate)\n",
    ")\n",
    "num_points = len(fetcher.x_train)\n",
    "covar_dim = (1,) if fetcher.x_train.ndim == 1 else fetcher.x_train[0].shape\n",
    "label_dim = (1,) if fetcher.y_train.ndim == 1 else fetcher.y_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import examples of appropriate Models\n",
    "# TODO think of more Regression models\n",
    "from dataoob.model.mlp import RegressionMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = {\n",
    "    # Wrappers for sklearn modles, makes the api more cohesive\n",
    "    'ann': RegressionMLP(*covar_dim, *label_dim, layers=5, hidden_dim=25).to(device),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a model from the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ann\"\n",
    "metric_name = \"mse\"\n",
    "train_kwargs = train_kwargs = {\"epochs\": 50, \"batch_size\": 10} if model_name in (\"ann\", \"logreg\") else {}\n",
    "\n",
    "pred_model = catalog[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base line performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataoob.evaluator.api import metrics_dict\n",
    "model = pred_model.clone()\n",
    "x_train, y_train, *_, x_test, y_test = fetcher.datapoints\n",
    "model.fit(x_train, y_train, **train_kwargs)\n",
    "metric = metrics_dict[metric_name]\n",
    "\n",
    "metric(x_test, model.predict(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lots of imoprts for the many Data Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataoob.dataval.ame import AME\n",
    "from dataoob.dataval.dvrl import DVRL\n",
    "from dataoob.dataval.influence import InfluenceFunctionEval\n",
    "from dataoob.dataval.knnshap import KNNShapley\n",
    "from dataoob.dataval.oob import DataOob\n",
    "from dataoob.dataval.margcontrib import LeaveOneOut\n",
    "from dataoob.dataval.margcontrib import BetaShapley, DataShapley\n",
    "from dataoob.dataval.margcontrib.banzhaf import DataBanzhaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a series of data evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_evaluators = [\n",
    "    AME(num_models=1500, random_state=random_state),\n",
    "    DataOob(random_state=random_state),  # 1000 samples\n",
    "    DVRL(rl_epochs=3000, random_state=random_state, device=device),  # RL requires torch device\n",
    "    InfluenceFunctionEval(5000, random_state=random_state),\n",
    "    DataBanzhaf(5000, random_state=random_state),\n",
    "    BetaShapley(gr_threshold=1.05, min_samples=500, cache_name=\"cached\", random_state=random_state),\n",
    "    DataShapley(gr_threshold=1.05, min_samples=500, cache_name=\"cached\", random_state=random_state),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataoob.evaluator import ExperimentMediator\n",
    "exper_med = ExperimentMediator(\n",
    "    fetcher=fetcher,\n",
    "    data_evaluators=data_evaluators, \n",
    "    pred_model=pred_model,\n",
    "    train_kwargs=train_kwargs,\n",
    "    metric_name=metric_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments on the data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataoob.evaluator.exper_methods import (\n",
    "    discover_corrupted_sample,\n",
    "    noisy_detection,\n",
    "    remove_high_low,\n",
    ")\n",
    "\n",
    "# Saving the results\n",
    "import os\n",
    "output_dir = f\"tmp/{dataset_name}{noise_rate=}/{date}/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discover corrupted sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "df_resp, fig = exper_med.plot(discover_corrupted_sample, fig, col=2)\n",
    "df_resp.to_csv(f\"{output_dir}/discover_corrupted_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy detection F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resp = exper_med.evaluate(noisy_detection)\n",
    "df_resp.to_csv(f\"{output_dir}/noisy_detection.csv\")\n",
    "df_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing high values and low values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "df_resp, fig = exper_med.plot(remove_high_low, include_train=True, col=2)\n",
    "df_resp.to_csv(f\"{output_dir}/remove_high_low.csv\")\n",
    "df_resp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
