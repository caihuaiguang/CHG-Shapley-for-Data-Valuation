{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Demo "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up global variables and random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from dataoob.util import set_random_state\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "random_state = set_random_state(10)\n",
    "date = datetime.now().strftime(\"%m-%d_%H:%M\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick Noise rate and data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_rate = .1 \n",
    "classification_datasets = [\"iris\", \"adult\", \"digits\", \"gaussian_classifier\"]\n",
    "dataset_name = classification_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataoob.dataloader import DataFetcher, mix_labels\n",
    "\n",
    "# Equivalent arguments\n",
    "fetcher = (\n",
    "    DataFetcher(dataset_name, False, random_state)\n",
    "    .split_dataset(100, 50, 50)\n",
    "    .noisify(mix_labels, noise_rate=noise_rate)\n",
    ")\n",
    "num_points = len(fetcher.x_train)\n",
    "covar_dim = (1,) if fetcher.x_train.ndim == 1 else fetcher.x_train[0].shape\n",
    "label_dim = (1,) if fetcher.y_train.ndim == 1 else fetcher.y_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import examples of appropriate Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from dataoob.model import ClassifierSkLearnWrapper, ClassifierUnweightedSkLearnWrapper\n",
    "from dataoob.model.logistic_regression import LogisticRegression as LR, BinaryLogisticRegression as BLR\n",
    "from dataoob.model.ann import ClassifierMLP, BinaryMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = {\n",
    "    # Wrappers for sklearn modles, makes the api more cohesive\n",
    "    'sklogreg': ClassifierSkLearnWrapper(LogisticRegression(), *label_dim, device=device),\n",
    "    'skknn': ClassifierUnweightedSkLearnWrapper(KNeighborsClassifier(*label_dim), *label_dim, device=device),\n",
    "\n",
    "    'logreg': LR(*covar_dim, *label_dim).to(device),\n",
    "    'ann': ClassifierMLP(*covar_dim, *label_dim, layers=5, hidden_dim=25).to(device),\n",
    "\n",
    "    'binlogreg': BLR(*covar_dim).to(device),\n",
    "    'binann': BinaryMLP(*covar_dim, layers=5, hidden_dim=25).to(device),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a model from the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ann\"\n",
    "metric_name = \"accuracy\"\n",
    "train_kwargs = train_kwargs = {\"epochs\": 20, \"batch_size\": 10} if model_name in (\"ann\", \"logreg\") else {}\n",
    "\n",
    "pred_model = catalog[model_name]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base line performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataoob.evaluator.api import metrics_dict\n",
    "model = pred_model.clone()\n",
    "x_train, y_train, x_valid, y_valid, *_ = fetcher.datapoints\n",
    "model.fit(x_train, y_train, **train_kwargs)\n",
    "metric = metrics_dict[metric_name]\n",
    "\n",
    "metric(y_valid, model.predict(x_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Evaluators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lots of imoprts for the many Data Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataoob.dataval.ame import AME\n",
    "from dataoob.dataval.dvrl import DVRL\n",
    "from dataoob.dataval.influence import InfluenceFunctionEval\n",
    "from dataoob.dataval.knnshap import KNNShapley\n",
    "from dataoob.dataval.oob import DataOob\n",
    "from dataoob.dataval.margcontrib import LeaveOneOut\n",
    "from dataoob.dataval.margcontrib import BetaShapley, DataShapley\n",
    "from dataoob.dataval.margcontrib.banzhaf import DataBanzhaf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a series of data evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_evaluators = [\n",
    "    AME(num_models=1500, random_state=random_state),\n",
    "    DataOob(random_state=random_state),  # 1000 samples\n",
    "    DVRL(rl_epochs=3000, random_state=random_state, device=device),  # RL requires torch device\n",
    "    InfluenceFunctionEval(5000, random_state=random_state),\n",
    "    DataBanzhaf(5000, random_state=random_state),\n",
    "    BetaShapley(gr_threshold=1.05, min_samples=500, cache_name=\"cached\", random_state=random_state),\n",
    "    DataShapley(gr_threshold=1.05, min_samples=500, cache_name=\"cached\", random_state=random_state),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataoob.evaluator import ExperimentMediator\n",
    "exper_med = ExperimentMediator(\n",
    "    fetcher=fetcher,\n",
    "    data_evaluators=data_evaluators, \n",
    "    pred_model=pred_model,\n",
    "    train_kwargs=train_kwargs,\n",
    "    metric_name=metric_name\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments on the data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataoob.evaluator.exper_methods import (\n",
    "    discover_corrupted_sample, \n",
    "    noisy_detection, \n",
    "    remove_high_low, \n",
    "    point_removal\n",
    ")\n",
    "\n",
    "# Saving the results\n",
    "import os\n",
    "output_dir = f\"tmp/{dataset_name}{noise_rate=}/{date}/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discover corrupted sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "df_resp = exper_med.plot(discover_corrupted_sample, fig, col=2)\n",
    "df_resp[0].to_csv(f\"{output_dir}/discover_corrupted_sample.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy detection F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resp = exper_med.evaluate(noisy_detection)\n",
    "df_resp.to_csv(f\"{output_dir}/noisy_detection.csv\")\n",
    "df_resp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing high values and low values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "df_resp, fig = exper_med.plot(remove_high_low, include_train=True, col=2)\n",
    "df_resp.to_csv(f\"{output_dir}/remove_high_low.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove descending values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "df_resp, fig = exper_med.plot(point_removal, include_train=True, col=2, percentile=.05, order=\"descending\")\n",
    "df_resp.to_csv(f\"{output_dir}/descending_remove.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove ascending values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "df_resp, fig = exper_med.plot(point_removal, include_train=True, col=2, order=\"ascending\")\n",
    "df_resp.to_csv(f\"{output_dir}/ascending_remove.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
