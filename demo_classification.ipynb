{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Demo "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up global variables and random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from opendataval.util import set_random_state\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "random_state = set_random_state(10)\n",
    "date = datetime.now().strftime(\"%m-%d_%H:%M\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_datasets = [\"iris\", \"adult\", \"digits\", \"gaussian_classifier\"]\n",
    "classification_models = ['logreg', 'mlpclass', \"skmlp\", \"sklogreg\", \"skknn\"]\n",
    "\n",
    "dataset_name = classification_datasets[1]\n",
    "train_count, valid_count, test_count = 100, 50, 50\n",
    "noise_rate = 0.1\n",
    "noise_kwargs = {'noise_rate': noise_rate }\n",
    "\n",
    "\n",
    "model_name = classification_models[4]\n",
    "train_kwargs = {\"epochs\": 20, \"batch_size\": 50} if \"sk\" not in model_name else {}\n",
    "metric_name = \"accuracy\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ExperimentMediator without specifying DataEvaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opendataval.experiment import ExperimentMediator\n",
    "\n",
    "exper_med = ExperimentMediator.model_factory_setup(\n",
    "    dataset_name=dataset_name,\n",
    "    force_download=False,\n",
    "    train_count=train_count,\n",
    "    valid_count=valid_count,\n",
    "    test_count=test_count,\n",
    "    noise_kwargs=noise_kwargs,\n",
    "    random_state=random_state,\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    train_kwargs=train_kwargs,\n",
    "    metric_name=metric_name\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Evaluators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lots of imports for the many Data Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opendataval.dataval.ame import AME\n",
    "from opendataval.dataval.dvrl import DVRL\n",
    "from opendataval.dataval.influence import InfluenceFunctionEval\n",
    "from opendataval.dataval.knnshap import KNNShapley\n",
    "from opendataval.dataval.oob import DataOob\n",
    "from opendataval.dataval.margcontrib import LeaveOneOut\n",
    "from opendataval.dataval.margcontrib import BetaShapley, DataShapley\n",
    "from opendataval.dataval.margcontrib.banzhaf import DataBanzhaf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a series of data evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_evaluators = [\n",
    "    AME(num_models=1500, random_state=random_state),\n",
    "    DataOob(2000,random_state=random_state),  # 1000 samples\n",
    "    DVRL(rl_epochs=4000, random_state=random_state, device=device),  # RL requires torch device\n",
    "    InfluenceFunctionEval(5000, random_state=random_state),\n",
    "    DataBanzhaf(5000, random_state=random_state),\n",
    "    BetaShapley(gr_threshold=1.05, min_samples=500, cache_name=\"cached\", random_state=random_state),\n",
    "    DataShapley(gr_threshold=1.05, min_samples=500, cache_name=\"cached\", random_state=random_state),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exper_med = exper_med.compute_data_values(data_evaluators=data_evaluators)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments on the data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opendataval.experiment.exper_methods import (\n",
    "    discover_corrupted_sample,\n",
    "    noisy_detection,\n",
    "    remove_high_low,\n",
    "    increasing_bin_removal,\n",
    "    save_dataval\n",
    ")\n",
    "\n",
    "# Saving the results\n",
    "import os\n",
    "output_dir = f\"tmp/{dataset_name}{noise_rate=}/{date}/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discover corrupted sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "df_resp = exper_med.plot(discover_corrupted_sample, fig, col=2)\n",
    "df_resp[0].to_csv(f\"{output_dir}/discover_corrupted_sample.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy detection F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resp = exper_med.evaluate(noisy_detection)\n",
    "df_resp.to_csv(f\"{output_dir}/noisy_detection.csv\")\n",
    "df_resp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove High Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "df_resp = exper_med.plot(remove_high_low, fig, include_train=True, col=2)\n",
    "df_resp[0].to_csv(f\"{output_dir}/remove_high_low.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing Bin removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "df_resp = exper_med.plot(increasing_bin_removal, fig, include_train=True, col=2)\n",
    "df_resp[0].to_csv(f\"{output_dir}/increasing_bin_removal.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resp = exper_med.evaluate(save_dataval)\n",
    "df_resp.to_csv(f\"{output_dir}/save_dataval.csv\")\n",
    "df_resp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
