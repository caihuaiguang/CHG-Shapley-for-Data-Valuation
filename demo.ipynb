{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo of current progress with Dataoob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dataoob.util import set_random_state\n",
    "device = torch.device(\"cpu\")\n",
    "random_state = set_random_state(10)\n",
    "date = datetime.now().strftime(\"%m-%d_%H:%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataoob.dataloader.fetcher import DataFetcher, mix_labels\n",
    "from dataoob.evaluator import ExperimentMediator, DataFetcherArgs\n",
    "\n",
    "dataset_name = \"iris\"\n",
    "noise_rate = .1\n",
    "\n",
    "# Equivalent arguments\n",
    "fetcher = (\n",
    "    DataFetcher(dataset_name, False, random_state)\n",
    "    .split_dataset(80, 30)\n",
    "    .noisify(mix_labels, noise_rate=noise_rate)\n",
    ")\n",
    "num_points = len(fetcher.x_train)\n",
    "covar_dim = len(fetcher.x_train[0])\n",
    "label_dim = fetcher.y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the models and default arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from dataoob.model import ClassifierSkLearnWrapper, ClassifierUnweightedSkLearnWrapper\n",
    "from dataoob.model.logistic_regression import LogisticRegression as LR\n",
    "from dataoob.model.mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    # Wrappers for sklearn modles, makes the api more cohesive\n",
    "    'sklogreg': ClassifierSkLearnWrapper(LogisticRegression(), label_dim, device=device),\n",
    "    'logreg': LR(covar_dim, label_dim).to(device),\n",
    "    'ann': MLP(covar_dim, label_dim, layers=3, hidden_dim=15).to(device),\n",
    "    'skknn': ClassifierUnweightedSkLearnWrapper(KNeighborsClassifier(label_dim), label_dim, device=device)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting your metrics and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataoob.evaluator import DataEvaluatorArgs\n",
    "model_name = \"ann\"\n",
    "metric_name = \"accuracy\"\n",
    "train_kwargs = {\"epochs\": 10, \"batch_size\": 20} if model_name in (\"ann\", \"logreg\") else {}\n",
    "\n",
    "de_args = DataEvaluatorArgs(\n",
    "    pred_model=models[model_name],\n",
    "    metric_name=metric_name,\n",
    "    train_kwargs=train_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base line model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataoob.evaluator.api import metrics_dict\n",
    "model = models[model_name].clone()\n",
    "x_train, y_train, x_valid, y_valid, *_ = fetcher.datapoints\n",
    "model.fit(x_train, y_train, **train_kwargs)\n",
    "metric = metrics_dict[metric_name]\n",
    "\n",
    "metric(y_valid, model.predict(x_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Evaluators present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataoob.dataval.influence import InfluenceFunctionEval\n",
    "from dataoob.dataval.dvrl import DVRL\n",
    "from dataoob.dataval.margcontrib import LeaveOneOut\n",
    "from dataoob.dataval.oob import DataOob\n",
    "from dataoob.dataval.knnshap import KNNShapley\n",
    "from dataoob.dataval.margcontrib import DataShapley\n",
    "from dataoob.dataval.margcontrib import BetaShapley\n",
    "from dataoob.dataval.margcontrib.banzhaf import DataBanzhaf, DataBanzhafMargContrib\n",
    "from dataoob.dataval.ame import BaggingEvaluator, AME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummy_eval = [  # Used for quick testing and run throughs\n",
    "    InfluenceFunctionEval(10, random_state=random_state),\n",
    "    DataOob(10, random_state=random_state),\n",
    "    DVRL(10, rl_epochs=10, random_state=random_state),\n",
    "    LeaveOneOut(random_state=random_state),\n",
    "    AME(10, random_state=random_state),\n",
    "    DataBanzhaf(samples=10, random_state=random_state),\n",
    "    BetaShapley(100, min_samples=99, model_name=\"t\", random_state=random_state),\n",
    "    DataShapley(model_name=\"t\", random_state=random_state),\n",
    "    DataShapley(100, min_samples=99, model_name=\"r\", random_state=random_state),\n",
    "]\n",
    "\n",
    "data_evaluators = [  # actual run through of experiments, will take long time \n",
    "    InfluenceFunctionEval(2000, random_state=random_state),\n",
    "    DataOob(random_state=random_state),\n",
    "    DVRL(rl_epochs=2000, random_state=random_state),\n",
    "    LeaveOneOut(random_state=random_state),\n",
    "    AME(random_state=random_state),\n",
    "    DataBanzhaf(10000, random_state=random_state),\n",
    "    DataBanzhafMargContrib(gr_threshold=1.05, min_samples=500, model_name=\"t\", random_state=random_state),\n",
    "    BetaShapley(gr_threshold=1.05, min_samples=500, model_name=\"t\", random_state=random_state),\n",
    "    DataShapley(gr_threshold=1.05, min_samples=500, model_name=\"t\", random_state=random_state),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the Evaluator Mediator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exper_med = ExperimentMediator(fetcher, data_evaluators, de_args.pred_model, de_args.train_kwargs, de_args.metric_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting and getting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from dataoob.evaluator.exper_methods import (\n",
    "    discover_corrupted_sample, noisy_detection, remove_high_low\n",
    ")\n",
    "import os\n",
    "output_dir = f\"tmp/{dataset_name}{noise_rate=}/{date}/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "df_resp = exper_med.plot(discover_corrupted_sample, fig, col=2)\n",
    "df_resp[0].to_csv(f\"{output_dir}/discover_corrupted_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_resp = exper_med.evaluate(noisy_detection)\n",
    "df_resp.to_csv(f\"{output_dir}/noisy_detection.csv\")\n",
    "df_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "df_resp, fig = exper_med.plot(remove_high_low, include_train=True, col=2)\n",
    "df_resp.to_csv(f\"{output_dir}/remove_high_low.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_saved_csv(file_path: str):\n",
    "    return pd.read_csv(file_path, index_col=[0, 1])\n",
    "read_saved_csv(f\"{output_dir}/discover_corrupted_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a30792999e96d101ca76d9a040890fe347a625eabba526b17f36b2f64aabff3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
